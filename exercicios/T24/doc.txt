1) Tempo programa somatório em cuda:
real	0m1.652s
user	0m0.538s
sys	    0m1.028s


2) Tempo gasto nas funções [CUDA memcpy HtoD] e sum_cuda(double*, double*, int) 

==10061== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max           Name
95.41%     462.64ms     1       462.64ms  462.64ms  462.64ms  [CUDA memcpy HtoD]
 4.51%     21.872ms     1       21.872ms  21.872ms  21.872ms  sum_cuda(double*, double*, int)

3) Tempo sequêncial
real    0m0.557s
user    0m0.334s
sys     0m0.219s

4) Tempo paralelo multicore com OpenMP
real    0m0.230s
user    0m0.491s
sys     0m0.336s

5) Tempo paralelo GPU com OpenMP
O tempo aumentou por causa do overhead da transferência de dados do vetor para a gpu.
real    0m2.285s
user    0m1.318s
sys     0m1.219s

6) Tempo somatório em CUDA sem __shared__
real    0m1.932s
user    0m0.725s
sys     0m1.102s


==10877== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 93.52%  463.44ms         1  463.44ms  463.44ms  463.44ms  [CUDA memcpy HtoD]
  6.41%  31.744ms         1  31.744ms  31.744ms  31.744ms  sum_cuda(double*, double*, int)

Comentários:

OpenMP com GPU x CUDA -> os tempos com CUDA são melhores pois é possível utilizar mais threads
que o OpenMP. Devido limitação da última, muitas threads são desperdiçadas com operações redundantes.

Com __shared__ x sem __shared__ -> Os tempos com shared são melhores pois aproveita-se da memória
dos blocos que possuem acesso mais rápido do que a memória global.

[CUDA memcpy HtoD] e sum_cuda(double*, double*, int) -> O maior overhead do programa é levar os dados
do Host para o Device e não os cáculos em si. Por isso as versões sequêncial e multicore têm tempos 
muito melhores.
